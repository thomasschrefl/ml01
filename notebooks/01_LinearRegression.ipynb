{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44336d36",
   "metadata": {},
   "source": [
    "# What is EDA (Exploratory Data Analysis)?\n",
    "\n",
    "EDA is the first step in any data science or machine learning workflow.\n",
    "\n",
    "**Purpose:**\n",
    "- Understand the data before modeling.\n",
    "- Detect patterns, trends, anomalies, and relationships.\n",
    "- Check distributions, scales, and potential outliers.\n",
    "\n",
    "**Common EDA tools:**\n",
    "- Scatter plots (relationships).\n",
    "- Histograms (distribution).\n",
    "- Summary statistics (`pandas.DataFrame.describe()`).\n",
    "- Correlation matrices.\n",
    "\n",
    "**Best practice:** Perform EDA on the **training set only** to avoid leaking information from the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cb4f6",
   "metadata": {},
   "source": [
    "# Exercise: EDA â†’ Heteroscedastic Linear Data (600 pts) â†’ Train/Test â†’ OP vs PO + Residual fits + Metrics\n",
    "\n",
    "This notebook walks through: (1) split-first EDA, (2) linear model fitting with scikit-learn, (3) proper evaluation on the test set with OP vs PO plots, (4) metrics (MAE/MSE/RÂ²), and (5) residual diagnostics with fitted trend lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements: numpy, pandas, matplotlib, scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = False\n",
    "print('Imports OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a6d55",
   "metadata": {},
   "source": [
    "## 1) Create the data (heteroscedastic, N=600)\n",
    "Noise standard deviation increases linearly from 5 to 50 as x grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 600\n",
    "rng = np.random.default_rng(seed=42)\n",
    "X = np.arange(1, N + 1, dtype=float).reshape(-1, 1)\n",
    "w = (X.ravel() - X.min()) / (X.max() - X.min())\n",
    "sigma_min, sigma_max = 5.0, 50.0\n",
    "sigma = sigma_min + (sigma_max - sigma_min) * w\n",
    "eps = rng.normal(loc=0.0, scale=sigma, size=N)\n",
    "Y = X.ravel() + eps\n",
    "X[:5].ravel(), Y[:5], sigma[:5]  # peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d7f01",
   "metadata": {},
   "source": [
    "## 2) Train/Test Split\n",
    "Fit on **train**, evaluate on **test**. EDA below uses **train only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, sigma_train, sigma_test = train_test_split(\n",
    "    X, Y, sigma, test_size=0.30, random_state=123, shuffle=True\n",
    ")\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77c497",
   "metadata": {},
   "source": [
    "## 3) EDA (train only): scatter, histogram, and `pandas.DataFrame.describe()`\n",
    "Keep it simple and fast; the goal is to understand scale, distribution, and relationship before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc88636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small DataFrame for TRAIN\n",
    "df_train = pd.DataFrame({'X': X_train.ravel(), 'y': y_train})\n",
    "\n",
    "# 3A) Scatter: X vs y (train)\n",
    "plt.figure(figsize=(6.8, 4.2))\n",
    "plt.scatter(df_train['X'], df_train['y'], s=10, alpha=0.7, color='tab:blue')\n",
    "plt.xlabel('X [train]')\n",
    "plt.ylabel('y [train]')\n",
    "plt.title('Training data: y vs X')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3B) Histogram of y (train)\n",
    "plt.figure(figsize=(6.2, 4.0))\n",
    "df_train['y'].plot(kind='hist', bins=30, color='tab:green', alpha=0.85, edgecolor='white')\n",
    "plt.xlabel('y [train]')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram: y (train)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3C) Simple statistical description with pandas\n",
    "display(df_train.describe())\n",
    "\n",
    "# 3D) Quick correlation (train)\n",
    "corr = df_train[['X','y']].corr().loc['X','y']\n",
    "print(f'TRAIN correlation corr(X,y) = {corr:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798b876",
   "metadata": {},
   "source": [
    "> **Callout: What does `pandas.DataFrame.describe()` show?**\n",
    ">> - **count**: number of non-null observations.\n",
    ">> - **mean / std**: central tendency and spread.\n",
    ">> - **min, 25%, 50%, 75%, max**: five-number summary.\n",
    ">> - Use it to quickly check scale, spread, and outliers/skew before modeling.\n",
    ">> ðŸ‘‰ *Best practice:* perform EDA on the **training set only** to avoid test leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257157aa",
   "metadata": {},
   "source": [
    "### About scikit-learn and LinearRegression\n",
    "`scikit-learn` is a widely used Python library for machine learning.\n",
    "- Provides tools for regression, classification, clustering, preprocessing, and model evaluation.\n",
    "- `LinearRegression` implements Ordinary Least Squares (OLS) for fitting a linear model: \n",
    "  $$ \\hat{y} = aX + b $$\n",
    "- Assumes a linear relationship between predictors and response.\n",
    "- In this exercise, we use it to fit a simple 1D linear model on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f557b4",
   "metadata": {},
   "source": [
    "## 4) Fit a linear model on TRAIN only\n",
    "We fit $\\hat Y = aX + b$ using ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "a, b = lin.coef_[0], lin.intercept_\n",
    "print(f'Fit on TRAIN: Y_hat = {a:.3f} * X + {b:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004e5fc",
   "metadata": {},
   "source": [
    "## 5) Predict on TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b264c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lin.predict(X_test)\n",
    "yhat_test[:5]  # peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddc68d",
   "metadata": {},
   "source": [
    "### Plot Interpretation\n",
    "- **OP plot (Observed vs Predicted):**\n",
    "  - Observed values on y-axis, predicted on x-axis.\n",
    "  - Overlay 1:1 line: ideal predictions fall on this line.\n",
    "  - OP regression line close to 1:1 indicates unbiased predictions.\n",
    "- **PO plot (Predicted vs Observed):**\n",
    "  - Swapping axes distorts slope/intercept; misleading for bias checks.\n",
    "- **Residual plots:**\n",
    "  - Residuals vs predicted (correct): should center around 0; spread pattern reveals heteroscedasticity.\n",
    "  - Residuals vs true (pitfall): can show spurious trends because y contains noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0980c8",
   "metadata": {},
   "source": [
    "## 6) OP plot (Observed vs Predicted) â€” **correct** evaluation on TEST\n",
    "Scatter **y** (y-axis) vs **\\^y** (x-axis); overlay the 1:1 line and the OP regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9da5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(x, y):\n",
    "    lr = LinearRegression().fit(x.reshape(-1,1), y)\n",
    "    return lr.coef_[0], lr.intercept_, lr.predict(x.reshape(-1,1))\n",
    "\n",
    "slope_op, intercept_op, y_op_line = fit_line(yhat_test, y_test)\n",
    "xy_min = min(y_test.min(), yhat_test.min())\n",
    "xy_max = max(y_test.max(), yhat_test.max())\n",
    "grid = np.linspace(xy_min, xy_max, 100)\n",
    "\n",
    "plt.figure(figsize=(6.2, 4.4))\n",
    "plt.scatter(yhat_test, y_test, s=20, alpha=0.7, label='test points')\n",
    "plt.plot(grid, grid, 'k--', lw=1.3, label='1:1')\n",
    "plt.plot(yhat_test, y_op_line, color='tab:green', lw=2, label=f'OP fit: y = {slope_op:.2f}Â·Å· + {intercept_op:.1f}')\n",
    "plt.xlabel('Predicted (Å·) [test]')\n",
    "plt.ylabel('Observed (y) [test]')\n",
    "plt.title('Evaluation (OP, correct) on TEST â€” heteroscedastic')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8845f38",
   "metadata": {},
   "source": [
    "### Performance Indicators Explained\n",
    "- **MAE (Mean Absolute Error):** Average absolute difference between predictions and true values.\n",
    "  - Easy to interpret; less sensitive to outliers than MSE.\n",
    "- **MSE (Mean Squared Error):** Average squared difference between predictions and true values.\n",
    "  - Penalizes large errors more heavily.\n",
    "- **RÂ² (Coefficient of Determination):** Proportion of variance in observed values explained by predictions.\n",
    "  - Ranges from 0 to 1 (higher is better).\n",
    "  - Can be negative if the model performs worse than predicting the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fc8df",
   "metadata": {},
   "source": [
    "## 6b) Performance metrics on TEST (scikit-learn)\n",
    "Compute **MAE**, **MSE**, and **RÂ²** on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, yhat_test)\n",
    "mse = mean_squared_error(y_test, yhat_test)\n",
    "r2  = r2_score(y_test, yhat_test)\n",
    "print(f'MAE (test): {mae:.3f}')\n",
    "print(f'MSE (test): {mse:.3f}')\n",
    "print(f'R^2 (test): {r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954200f",
   "metadata": {},
   "source": [
    "## 7) PO plot (Predicted vs Observed) â€” **pitfall** on TEST\n",
    "Swapping axes changes slope/intercept even if $R^2$ stays the same; evaluating with PO can lead to biased conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_po, intercept_po, y_po_line = fit_line(y_test, yhat_test)\n",
    "\n",
    "plt.figure(figsize=(6.2, 4.4))\n",
    "plt.scatter(y_test, yhat_test, s=20, alpha=0.7, label='test points')\n",
    "plt.plot(grid, grid, 'k--', lw=1.3, label='1:1')\n",
    "plt.plot(y_test, y_po_line, color='tab:orange', lw=2, label=f'PO fit: Å· = {slope_po:.2f}Â·y + {intercept_po:.1f}')\n",
    "plt.xlabel('Observed (y) [test]')\n",
    "plt.ylabel('Predicted (Å·) [test]')\n",
    "plt.title('Evaluation (PO, pitfall) on TEST â€” heteroscedastic')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212be0fa",
   "metadata": {},
   "source": [
    "## 8) Residual plots on TEST **with fitted trend lines**\n",
    "- **Correct**: residuals vs predicted (Å·) â€” center should be ~0; spread increases.\n",
    "- **Pitfall**: residuals vs true (y) â€” misleading structure may appear since y contains noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = y_test - yhat_test\n",
    "\n",
    "# Residuals vs Predicted (correct) + fitted trend line\n",
    "slope_rp, intercept_rp, res_fit_pred = fit_line(yhat_test, res_test)\n",
    "plt.figure(figsize=(6.2, 3.8))\n",
    "plt.scatter(yhat_test, res_test, s=18, alpha=0.7, label='residuals')\n",
    "plt.axhline(0, color='k', lw=1)\n",
    "plt.plot(yhat_test, res_fit_pred, color='tab:red', lw=2, label=f'fit: e = {slope_rp:.3f}Â·Å· + {intercept_rp:.2f}')\n",
    "plt.xlabel('Predicted (Å·) [test]')\n",
    "plt.ylabel('Residual e = y - Å· [test]')\n",
    "plt.title('Residuals vs Predicted (correct) â€” with fitted trend')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Residuals vs True (pitfall) + fitted trend line\n",
    "slope_rt, intercept_rt, res_fit_true = fit_line(y_test, res_test)\n",
    "plt.figure(figsize=(6.2, 3.8))\n",
    "plt.scatter(y_test, res_test, s=18, alpha=0.7, label='residuals')\n",
    "plt.axhline(0, color='k', lw=1)\n",
    "plt.plot(y_test, res_fit_true, color='tab:purple', lw=2, label=f'fit: e = {slope_rt:.3f}Â·y + {intercept_rt:.2f}')\n",
    "plt.xlabel('Observed (y) [test]')\n",
    "plt.ylabel('Residual e = y - Å· [test]')\n",
    "plt.title('Residuals vs True (pitfall) â€” with fitted trend')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27db6a-4ef6-4b3a-a71c-bac33d40bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
