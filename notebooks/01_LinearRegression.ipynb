{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44336d36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section 1: What is EDA?\n",
    "\n",
    "**Explanation**  \n",
    "Exploratory Data Analysis (EDA) is the first step in any data science workflow. It helps you understand the dataset before modeling. You check distributions, relationships, and anomalies to avoid surprises later.\n",
    "\n",
    "**Why important?**  \n",
    "- Detect patterns and outliers.  \n",
    "- Understand scale and variability.  \n",
    "- Prevent data leakage (do EDA on training set only).  \n",
    "\n",
    "**Common tools**  \n",
    "- Scatter plots → relationships.\n",
    "- Histograms → distribution.\n",
    "- pandas.DataFrame.describe() → summary stats.\n",
    "- Correlation matrices → linear relationships.\n",
    "\n",
    "**Questions**  \n",
    "Why should EDA be done only on the training set?    \n",
    "What does a correlation coefficient close to 1 mean?    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cb4f6",
   "metadata": {},
   "source": [
    "# Section 2: Heteroscedastic Data  \n",
    "**Explanation**  \n",
    "We simulate data where noise increases with X (heteroscedasticity). This is common in real-world scenarios (e.g., measurement error grows with magnitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements: numpy, pandas, matplotlib, scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = False\n",
    "print('Imports OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a6d55",
   "metadata": {},
   "source": [
    "#### Create the data (heteroscedastic, N=600)\n",
    "Noise standard deviation increases linearly from 5 to 50 as x grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 600\n",
    "rng = np.random.default_rng(seed=42)\n",
    "X = np.arange(1, N + 1, dtype=float).reshape(-1, 1)\n",
    "w = (X.ravel() - X.min()) / (X.max() - X.min())\n",
    "sigma_min, sigma_max = 5.0, 50.0\n",
    "sigma = sigma_min + (sigma_max - sigma_min) * w\n",
    "eps = rng.normal(loc=0.0, scale=sigma, size=N)\n",
    "Y = X.ravel() + eps\n",
    "X[:5].ravel(), Y[:5], sigma[:5]  # peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d7f01",
   "metadata": {},
   "source": [
    "**Task**  \n",
    "- Plot sigma vs X. What do you observe?\n",
    "- Why does increasing noise make modeling harder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7207d-4393-49d7-b81c-ab7b0ef76383",
   "metadata": {},
   "source": [
    "## Section 3: Train/Test Split\n",
    "\n",
    "**Explanation** \n",
    "We split data into training and test sets to evaluate generalization.  \n",
    "*Rule:* Never peek at test data during training or EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, sigma_train, sigma_test = train_test_split(\n",
    "    X, Y, sigma, test_size=0.30, random_state=123, shuffle=True\n",
    ")\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77c497",
   "metadata": {},
   "source": [
    "**Question**  \n",
    "What happens if you do EDA on the full dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844c4dd-52b7-4f62-830a-d74ce909729e",
   "metadata": {},
   "source": [
    "## Section 4: EDA on training data  \n",
    "**Explanation**  \n",
    "Scatter plots show relationships. Histograms reveal distribution and skew. describe() gives quick stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc88636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small DataFrame for TRAIN\n",
    "df_train = pd.DataFrame({'X': X_train.ravel(), 'y': y_train})\n",
    "\n",
    "# 3A) Scatter: X vs y (train)\n",
    "plt.figure(figsize=(6.8, 4.2))\n",
    "plt.scatter(df_train['X'], df_train['y'], s=10, alpha=0.7, color='tab:blue')\n",
    "plt.xlabel('X [train]')\n",
    "plt.ylabel('y [train]')\n",
    "plt.title('Training data: y vs X')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3B) Histogram of y (train)\n",
    "plt.figure(figsize=(6.2, 4.0))\n",
    "df_train['y'].plot(kind='hist', bins=30, color='tab:green', alpha=0.85, edgecolor='white')\n",
    "plt.xlabel('y [train]')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram: y (train)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3C) Simple statistical description with pandas\n",
    "display(df_train.describe())\n",
    "\n",
    "# 3D) Quick correlation (train)\n",
    "corr = df_train[['X','y']].corr().loc['X','y']\n",
    "print(f'TRAIN correlation corr(X,y) = {corr:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257157aa",
   "metadata": {},
   "source": [
    "**Task**  \n",
    "- Interpret the five-number summary.\n",
    "- Explain the meaning of reported correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d2ad2-e049-4d48-9dae-5a23c8d34e57",
   "metadata": {},
   "source": [
    "## Section 5: Fit Linear Model\n",
    "\n",
    "**Explanation:**\n",
    "We use LinearRegression from scikit-learn (OLS method).  \n",
    "\n",
    "`scikit-learn` is a widely used Python library for machine learning.\n",
    "- Provides tools for regression, classification, clustering, preprocessing, and model evaluation.\n",
    "- `LinearRegression` implements Ordinary Least Squares (OLS) for fitting a linear model: \n",
    "  $$ \\hat{y} = aX + b $$\n",
    "- Assumes a linear relationship between predictors and response.\n",
    "- In this exercise, we use it to fit a simple 1D linear model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "a, b = lin.coef_[0], lin.intercept_\n",
    "print(f'Fit on TRAIN: Y_hat = {a:.3f} * X + {b:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004e5fc",
   "metadata": {},
   "source": [
    "**Question**  \n",
    "Why does OLS minimize squared errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74bf62-db49-469b-8c22-a994fd187434",
   "metadata": {},
   "source": [
    "## Section 6: Evaluate on Test Set\n",
    "**Explanation**  \n",
    "- OP plot (Observed vs Predicted) is correct for bias check.\n",
    "- PO plot (Predicted vs Observed) is misleading.\n",
    "- Residual plots reveal heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b264c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lin.predict(X_test)\n",
    "yhat_test[:5]  # peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddc68d",
   "metadata": {},
   "source": [
    "##+# Plot Interpretation\n",
    "- **OP plot (Observed vs Predicted):**\n",
    "  - Observed values on y-axis, predicted on x-axis.\n",
    "  - Overlay 1:1 line: ideal predictions fall on this line.\n",
    "  - OP regression line close to 1:1 indicates unbiased predictions.\n",
    "- **PO plot (Predicted vs Observed):**\n",
    "  - Swapping axes distorts slope/intercept; misleading for bias checks.\n",
    "- **Residual plots:**\n",
    "  - Residuals vs predicted (correct): should center around 0; spread pattern reveals heteroscedasticity.\n",
    "  - Residuals vs true (pitfall): can show spurious trends because y contains noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0980c8",
   "metadata": {},
   "source": [
    "#### OP plot (Observed vs Predicted) — **correct** evaluation on TEST\n",
    "Scatter **y** (y-axis) vs **\\^y** (x-axis); overlay the 1:1 line and the OP regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9da5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(x, y):\n",
    "    lr = LinearRegression().fit(x.reshape(-1,1), y)\n",
    "    return lr.coef_[0], lr.intercept_, lr.predict(x.reshape(-1,1))\n",
    "\n",
    "slope_op, intercept_op, y_op_line = fit_line(yhat_test, y_test)\n",
    "xy_min = min(y_test.min(), yhat_test.min())\n",
    "xy_max = max(y_test.max(), yhat_test.max())\n",
    "grid = np.linspace(xy_min, xy_max, 100)\n",
    "\n",
    "plt.figure(figsize=(6.2, 4.4))\n",
    "plt.scatter(yhat_test, y_test, s=20, alpha=0.7, label='test points')\n",
    "plt.plot(grid, grid, 'k--', lw=1.3, label='1:1')\n",
    "plt.plot(yhat_test, y_op_line, color='tab:green', lw=2, label=f'OP fit: y = {slope_op:.2f}·ŷ + {intercept_op:.1f}')\n",
    "plt.xlabel('Predicted (ŷ) [test]')\n",
    "plt.ylabel('Observed (y) [test]')\n",
    "plt.title('Evaluation (OP, correct) on TEST — heteroscedastic')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8845f38",
   "metadata": {},
   "source": [
    "### Performance Indicators Explained\n",
    "- **MAE (Mean Absolute Error):** Average absolute difference between predictions and true values.\n",
    "  - Easy to interpret; less sensitive to outliers than MSE.\n",
    "- **MSE (Mean Squared Error):** Average squared difference between predictions and true values.\n",
    "  - Penalizes large errors more heavily.\n",
    "- **R² (Coefficient of Determination):** Proportion of variance in observed values explained by predictions.\n",
    "  - Ranges from 0 to 1 (higher is better).\n",
    "  - Can be negative if the model performs worse than predicting the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fc8df",
   "metadata": {},
   "source": [
    "#### Performance metrics on TEST (scikit-learn)\n",
    "Compute **MAE**, **MSE**, and **R²** on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, yhat_test)\n",
    "mse = mean_squared_error(y_test, yhat_test)\n",
    "r2  = r2_score(y_test, yhat_test)\n",
    "print(f'MAE (test): {mae:.3f}')\n",
    "print(f'MSE (test): {mse:.3f}')\n",
    "print(f'R^2 (test): {r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954200f",
   "metadata": {},
   "source": [
    "## Section 7: Residual Analysis\n",
    "Swapping axes changes slope/intercept even if $R^2$ stays the same; evaluating with PO can lead to biased conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_po, intercept_po, y_po_line = fit_line(y_test, yhat_test)\n",
    "\n",
    "plt.figure(figsize=(6.2, 4.4))\n",
    "plt.scatter(y_test, yhat_test, s=20, alpha=0.7, label='test points')\n",
    "plt.plot(grid, grid, 'k--', lw=1.3, label='1:1')\n",
    "plt.plot(y_test, y_po_line, color='tab:orange', lw=2, label=f'PO fit: ŷ = {slope_po:.2f}·y + {intercept_po:.1f}')\n",
    "plt.xlabel('Observed (y) [test]')\n",
    "plt.ylabel('Predicted (ŷ) [test]')\n",
    "plt.title('Evaluation (PO, pitfall) on TEST — heteroscedastic')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212be0fa",
   "metadata": {},
   "source": [
    "#### Residual plots on TEST **with fitted trend lines**\n",
    "- **Correct**: residuals vs predicted (ŷ) — center should be ~0; spread increases.\n",
    "- **Pitfall**: residuals vs true (y) — misleading structure may appear since y contains noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = y_test - yhat_test\n",
    "\n",
    "# Residuals vs Predicted (correct) + fitted trend line\n",
    "slope_rp, intercept_rp, res_fit_pred = fit_line(yhat_test, res_test)\n",
    "plt.figure(figsize=(6.2, 3.8))\n",
    "plt.scatter(yhat_test, res_test, s=18, alpha=0.7, label='residuals')\n",
    "plt.axhline(0, color='k', lw=1)\n",
    "plt.plot(yhat_test, res_fit_pred, color='tab:red', lw=2, label=f'fit: e = {slope_rp:.3f}·ŷ + {intercept_rp:.2f}')\n",
    "plt.xlabel('Predicted (ŷ) [test]')\n",
    "plt.ylabel('Residual e = y - ŷ [test]')\n",
    "plt.title('Residuals vs Predicted (correct) — with fitted trend')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Residuals vs True (pitfall) + fitted trend line\n",
    "slope_rt, intercept_rt, res_fit_true = fit_line(y_test, res_test)\n",
    "plt.figure(figsize=(6.2, 3.8))\n",
    "plt.scatter(y_test, res_test, s=18, alpha=0.7, label='residuals')\n",
    "plt.axhline(0, color='k', lw=1)\n",
    "plt.plot(y_test, res_fit_true, color='tab:purple', lw=2, label=f'fit: e = {slope_rt:.3f}·y + {intercept_rt:.2f}')\n",
    "plt.xlabel('Observed (y) [test]')\n",
    "plt.ylabel('Residual e = y - ŷ [test]')\n",
    "plt.title('Residuals vs True (pitfall) — with fitted trend')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04050364-5510-4cb2-84df-b0dcfbc06356",
   "metadata": {},
   "source": [
    "**Questions**  \n",
    "- Why is residual vs true misleading?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
