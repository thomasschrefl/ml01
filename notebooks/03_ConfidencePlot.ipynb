{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a014bc93",
   "metadata": {},
   "source": [
    "# Confidence plot\n",
    "\n",
    "**Goal**:\n",
    "1) Use the BaggingRegressor to compute the standard deviation of predictions\n",
    "2) Create the confidence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "from matminer.featurizers.composition import ElementProperty\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = False\n",
    "print('Imports OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e2057",
   "metadata": {},
   "source": [
    "## 1) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('..') / 'data' / 'DS1.csv'\n",
    "assert csv_path.exists(), f\"Could not find {csv_path}. Adjust the path if you moved the file.\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e15309",
   "metadata": {},
   "source": [
    "## 3) Train/Test split (use train for EDA and CV comparison)\n",
    "#### We also split the names for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = df['TC']\n",
    "names = df['Name'] \n",
    "X_orig_all = df.drop(columns=['TC']).select_dtypes(include=[np.number])\n",
    "mask = y_all.notna()\n",
    "y_all = y_all.loc[mask]\n",
    "X_orig_all = X_orig_all.loc[mask]\n",
    "X_orig_train, X_orig_test, y_train, y_test, names_train, names_test = train_test_split(\n",
    "    X_orig_all, y_all, names, test_size=0.30, random_state=123, shuffle=True\n",
    ")\n",
    "X_orig_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b013547",
   "metadata": {},
   "source": [
    "## 5) Fit with bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = BaggingRegressor(RandomForestRegressor(random_state = 123),\n",
    "                             n_estimators=20, random_state=123)\n",
    "reg.fit(X_orig_train, y_train)\n",
    "y_pred_orig = reg.predict(X_orig_test)\n",
    "X_test_np  = X_orig_test.to_numpy()  if hasattr(X_orig_test, \"to_numpy\")  else X_orig_test\n",
    "predictions = np.array([estimator.predict(X_test_np) for estimator in reg.estimators_])\n",
    "std = np.std(predictions, axis=0)\n",
    "def report(y_true, y_pred, label):\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'{label} - R^2: {r2:.4f}, MAE: {mae:.3f}')\n",
    "report(y_test, y_pred_orig, 'TEST RF (original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fada7-a59e-4eb7-b0b0-5d0e7c90c36e",
   "metadata": {},
   "source": [
    "## 6) Confidence plot\n",
    "We now have the following  \n",
    "*y_pred_orig* — ensemble prediction (mean over estimators)  \n",
    "*std* — the per‑sample uncertainty proxy (std over estimator predictions)  \n",
    "*y_test* — ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696e114-976c-4b40-bd05-8a1cd202755f",
   "metadata": {},
   "source": [
    "### Code and helper functions for the confidence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3cc37-0717-4858-9973-03dfae3bc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_np(x):\n",
    "    # Helper to make sure we have numpy arrays\n",
    "    return x.to_numpy().ravel() if hasattr(x, \"to_numpy\") else np.asarray(x).ravel()\n",
    "\n",
    "def confidence_curve(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    uncertainty,\n",
    "    metric: str = \"mae\",\n",
    "    step: int = 1,\n",
    "    min_remaining: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a confidence curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape (n_samples,)\n",
    "        Ground-truth targets.\n",
    "    y_pred : array-like, shape (n_samples,)\n",
    "        Model predictions.\n",
    "    uncertainty : array-like, shape (n_samples,)\n",
    "        Per-sample uncertainty estimates (higher = less confident).\n",
    "    metric : {\"mae\", \"rmse\"}\n",
    "        Error metric used to evaluate the remaining subset.\n",
    "    step : int\n",
    "        Number of samples to discard per iteration (use >1 for smoother/faster curves).\n",
    "    min_remaining : int\n",
    "        Stop discarding when fewer than this number of samples would remain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pct_discarded : np.ndarray\n",
    "        Percent of discarded samples at each step (0..100).\n",
    "    err_curve : np.ndarray\n",
    "        Error metric value on the remaining subset at each step.\n",
    "    \"\"\"\n",
    "    y_true = _as_np(y_true)\n",
    "    y_pred = _as_np(y_pred)\n",
    "    uncertainty = _as_np(uncertainty)\n",
    "\n",
    "    assert y_true.shape == y_pred.shape == uncertainty.shape\n",
    "\n",
    "    n = len(y_true)\n",
    "    order = np.argsort(-uncertainty)  # descending uncertainty\n",
    "    y_true_sorted = y_true[order]\n",
    "    y_pred_sorted = y_pred[order]\n",
    "\n",
    "    def compute_error(y_t, y_p):\n",
    "        if metric.lower() == \"mae\":\n",
    "            return mean_absolute_error(y_t, y_p)\n",
    "        elif metric.lower() == \"rmse\":\n",
    "            return np.sqrt(mean_squared_error(y_t, y_p))\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'mae' or 'rmse'\")\n",
    "\n",
    "    discards = list(range(0, max(0, n - min_remaining + 1), step))\n",
    "    pct_discarded = []\n",
    "    err_curve = []\n",
    "\n",
    "    for k in discards:\n",
    "        # Discard k most-uncertain samples: evaluate on the remaining tail\n",
    "        y_t_rem = y_true_sorted[k:]\n",
    "        y_p_rem = y_pred_sorted[k:]\n",
    "        if len(y_t_rem) < min_remaining:\n",
    "            break\n",
    "        pct_discarded.append(100.0 * k / n)\n",
    "        err_curve.append(compute_error(y_t_rem, y_p_rem))\n",
    "\n",
    "    return np.array(pct_discarded), np.array(err_curve)\n",
    "\n",
    "\n",
    "def ideal_curve(y_true, y_pred, metric: str = \"mae\", step: int = 1, min_remaining: int = 10):\n",
    "    \"\"\"\n",
    "    Oracle curve: same as confidence_curve but discard by true absolute error.\n",
    "    \"\"\"\n",
    "    y_true = _as_np(y_true)\n",
    "    y_pred = _as_np(y_pred)\n",
    "\n",
    "    abs_err = np.abs(y_true - y_pred)\n",
    "    # Highest actual error first\n",
    "    order = np.argsort(-abs_err)\n",
    "\n",
    "    n = len(y_true)\n",
    "    y_true_sorted = y_true[order]\n",
    "    y_pred_sorted = y_pred[order]\n",
    "\n",
    "    def compute_error(y_t, y_p):\n",
    "        if metric.lower() == \"mae\":\n",
    "            return mean_absolute_error(y_t, y_p)\n",
    "        elif metric.lower() == \"rmse\":\n",
    "            return np.sqrt(mean_squared_error(y_t, y_p))\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'mae' or 'rmse'\")\n",
    "\n",
    "    discards = list(range(0, max(0, n - min_remaining + 1), step))\n",
    "    pct_discarded = []\n",
    "    err_curve = []\n",
    "\n",
    "    for k in discards:\n",
    "        y_t_rem = y_true_sorted[k:]\n",
    "        y_p_rem = y_pred_sorted[k:]\n",
    "        if len(y_t_rem) < min_remaining:\n",
    "            break\n",
    "        pct_discarded.append(100.0 * k / n)\n",
    "        err_curve.append(compute_error(y_t_rem, y_p_rem))\n",
    "\n",
    "    return np.array(pct_discarded), np.array(err_curve)\n",
    "\n",
    "\n",
    "def plot_confidence_curve(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    uncertainty,\n",
    "    metric: str = \"mae\",\n",
    "    step: int = 1,\n",
    "    min_remaining: int = 10,\n",
    "    title: str = \"Confidence Curve\",\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and plot the confidence curve alongside the ideal (oracle) curve.\n",
    "    \"\"\"\n",
    "    pct_uq, err_uq = confidence_curve(\n",
    "        y_true, y_pred, uncertainty, metric=metric, step=step, min_remaining=min_remaining\n",
    "    )\n",
    "    pct_id, err_id = ideal_curve(\n",
    "        y_true, y_pred, metric=metric, step=step, min_remaining=min_remaining\n",
    "    )\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    ax.plot(pct_uq, err_uq, label=\"By predicted uncertainty\", color=\"#1f77b4\", lw=2)\n",
    "    ax.plot(pct_id, err_id, label=\"Ideal (by true error)\", color=\"#d62728\", lw=2, ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Discarded samples (%)\")\n",
    "    ylabel = \"MAE on remaining set\" if metric.lower() == \"mae\" \\\n",
    "             else \"RMSE on remaining set\"\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ad6e8-e4b4-47a2-9c0e-cbba8cf2f909",
   "metadata": {},
   "source": [
    "### Now we plot the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeed7df-bac0-493f-b5ef-b07ad3554ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure arrays (and consistent indexing with y_test)\n",
    "y_t = _as_np(y_test)\n",
    "y_p = _as_np(y_pred_orig)\n",
    "u   = _as_np(std)\n",
    "\n",
    "# Plot with MAE (default). Use step>1 for smoother curve on big test sets.\n",
    "ax = plot_confidence_curve(\n",
    "    y_true=y_t,\n",
    "    y_pred=y_p,\n",
    "    uncertainty=u,\n",
    "    metric=\"mae\",     # or \"rmse\"\n",
    "    step=5,           # discard in chunks of 5 samples for speed/smoothness\n",
    "    min_remaining=20, # don't evaluate on very tiny remainders\n",
    "    title=\"Bagging RF Confidence Curve (uncertainty = per-estimator std)\"\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
